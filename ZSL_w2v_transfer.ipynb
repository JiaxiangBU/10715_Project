{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Activation, AveragePooling2D, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras.datasets import cifar100\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ResNet56v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data and embeddings\n",
    "label_embeddings = pickle.load(open(\"Data/Embeddings/CIFAR/CIFAR_100_label_to_embedding_google_news.pk\", \"rb\"))\n",
    "embedding_len = len(label_embeddings[0])\n",
    "\n",
    "label_embeddings_arr = np.zeros((100, embedding_len))\n",
    "for i in range(100):\n",
    "    label_embeddings_arr[i] = label_embeddings[i]\n",
    "\n",
    "save_dir_feat = os.path.join(os.getcwd(), 'saved_models/zsl/%s/extracted_feat/' % (model_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_feat = np.load(os.path.join(save_dir_feat, 'X_train_feat_cifar100_%s.npy' % (model_type)))\n",
    "X_test_seen_feat = np.load(os.path.join(save_dir_feat, 'X_test_seen_feat_cifar100_%s.npy' % (model_type)))\n",
    "X_test_unseen_feat = np.load(os.path.join(save_dir_feat, 'X_test_unseen_feat_cifar100_%s.npy' % (model_type)))\n",
    "X_test_all_feat = np.load(os.path.join(save_dir_feat, 'X_test_all_feat_cifar100_%s.npy' % (model_type)))\n",
    "\n",
    "input_shape = X_train_feat[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.load(os.path.join(save_dir_feat, 'y_train_cifar100_%s.npy' % (model_type)))\n",
    "y_test_seen = np.load(os.path.join(save_dir_feat, 'y_test_seen_cifar100_%s.npy' % (model_type)))\n",
    "y_test_unseen = np.load(os.path.join(save_dir_feat, 'y_test_unseen_cifar100_%s.npy' % (model_type)))\n",
    "y_test_all = np.load(os.path.join(save_dir_feat, 'y_test_all_cifar100_%s.npy' % (model_type)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_embeddings = np.zeros((len(y_train), embedding_len))\n",
    "for i in range(len(y_train)):\n",
    "    y_train_embeddings[i] = label_embeddings[int(y_train[i])]\n",
    "    \n",
    "y_test_seen_embeddings = np.zeros((len(y_test_seen), embedding_len))\n",
    "for i in range(len(y_test_seen)):\n",
    "    y_test_seen_embeddings[i] = label_embeddings[int(y_test_seen[i])]\n",
    "    \n",
    "y_test_unseen_embeddings = np.zeros((len(y_test_unseen), embedding_len))\n",
    "for i in range(len(y_test_unseen)):\n",
    "    y_test_unseen_embeddings[i] = label_embeddings[int(y_test_unseen[i])]\n",
    "    \n",
    "y_test_all_embeddings = np.zeros((len(y_test_all), embedding_len))\n",
    "for i in range(len(y_test_all)):\n",
    "    y_test_all_embeddings[i] = label_embeddings[int(y_test_all[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=input_shape, embedding_len=embedding_len):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    x = BatchNormalization()(inputs)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    x = Dense(1024) (x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(512) (x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(embedding_len) (x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    outputs = Dense(embedding_len,\n",
    "                    kernel_initializer='he_normal')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              4195328   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               153900    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 300)               90300     \n",
      "=================================================================\n",
      "Total params: 4,972,696\n",
      "Trainable params: 4,968,512\n",
      "Non-trainable params: 4,184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(loss='cosine_proximity', optimizer='adam', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models/zsl/%s/w2v_transfer/' % (model_type))\n",
    "model_name = 'cifar100_%s_w2v_transfer_model.{epoch:03d}.h5' % (model_type)\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 120:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 40:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 20:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 8000 samples\n",
      "Epoch 1/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 7s 164us/step - loss: -0.6245 - mean_squared_error: 3.5680 - val_loss: -0.6549 - val_mean_squared_error: 2.5852\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to -0.65494, saving model to /home/tliu/Dev/CMU/10-715/10715_Project/saved_models/zsl/ResNet56v2/w2v_transfer/cifar100_ResNet56v2_w2v_transfer_model.001.h5\n",
      "Epoch 2/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.8003 - mean_squared_error: 3.6489 - val_loss: -0.6713 - val_mean_squared_error: 3.5071\n",
      "\n",
      "Epoch 00002: val_loss improved from -0.65494 to -0.67127, saving model to /home/tliu/Dev/CMU/10-715/10715_Project/saved_models/zsl/ResNet56v2/w2v_transfer/cifar100_ResNet56v2_w2v_transfer_model.002.h5\n",
      "Epoch 3/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 131us/step - loss: -0.8598 - mean_squared_error: 4.7316 - val_loss: -0.6854 - val_mean_squared_error: 5.4059\n",
      "\n",
      "Epoch 00003: val_loss improved from -0.67127 to -0.68536, saving model to /home/tliu/Dev/CMU/10-715/10715_Project/saved_models/zsl/ResNet56v2/w2v_transfer/cifar100_ResNet56v2_w2v_transfer_model.003.h5\n",
      "Epoch 4/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: -0.8916 - mean_squared_error: 5.4990 - val_loss: -0.6955 - val_mean_squared_error: 6.6516\n",
      "\n",
      "Epoch 00004: val_loss improved from -0.68536 to -0.69547, saving model to /home/tliu/Dev/CMU/10-715/10715_Project/saved_models/zsl/ResNet56v2/w2v_transfer/cifar100_ResNet56v2_w2v_transfer_model.004.h5\n",
      "Epoch 5/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 131us/step - loss: -0.9122 - mean_squared_error: 6.1307 - val_loss: -0.6937 - val_mean_squared_error: 7.0504\n",
      "\n",
      "Epoch 00005: val_loss did not improve from -0.69547\n",
      "Epoch 6/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: -0.9275 - mean_squared_error: 6.7087 - val_loss: -0.6910 - val_mean_squared_error: 6.4752\n",
      "\n",
      "Epoch 00006: val_loss did not improve from -0.69547\n",
      "Epoch 7/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: -0.9387 - mean_squared_error: 7.1757 - val_loss: -0.6870 - val_mean_squared_error: 7.1792\n",
      "\n",
      "Epoch 00007: val_loss did not improve from -0.69547\n",
      "Epoch 8/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 131us/step - loss: -0.9464 - mean_squared_error: 7.5938 - val_loss: -0.6813 - val_mean_squared_error: 7.2054\n",
      "\n",
      "Epoch 00008: val_loss did not improve from -0.69547\n",
      "Epoch 9/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 6s 139us/step - loss: -0.9544 - mean_squared_error: 7.9403 - val_loss: -0.6757 - val_mean_squared_error: 7.5812\n",
      "\n",
      "Epoch 00009: val_loss did not improve from -0.69547\n",
      "Epoch 10/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 123us/step - loss: -0.9585 - mean_squared_error: 8.2694 - val_loss: -0.6746 - val_mean_squared_error: 7.2026\n",
      "\n",
      "Epoch 00010: val_loss did not improve from -0.69547\n",
      "Epoch 11/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 134us/step - loss: -0.9627 - mean_squared_error: 8.5634 - val_loss: -0.6778 - val_mean_squared_error: 10.9776\n",
      "\n",
      "Epoch 00011: val_loss did not improve from -0.69547\n",
      "Epoch 12/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9651 - mean_squared_error: 8.8512 - val_loss: -0.6770 - val_mean_squared_error: 7.4320\n",
      "\n",
      "Epoch 00012: val_loss did not improve from -0.69547\n",
      "Epoch 13/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: -0.9671 - mean_squared_error: 9.1273 - val_loss: -0.6793 - val_mean_squared_error: 15.1757\n",
      "\n",
      "Epoch 00013: val_loss did not improve from -0.69547\n",
      "Epoch 14/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9707 - mean_squared_error: 9.4007 - val_loss: -0.6784 - val_mean_squared_error: 8.5574\n",
      "\n",
      "Epoch 00014: val_loss did not improve from -0.69547\n",
      "Epoch 15/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.9720 - mean_squared_error: 9.6501 - val_loss: -0.6621 - val_mean_squared_error: 11.9290\n",
      "\n",
      "Epoch 00015: val_loss did not improve from -0.69547\n",
      "Epoch 16/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: -0.9735 - mean_squared_error: 9.9062 - val_loss: -0.6788 - val_mean_squared_error: 8.5803\n",
      "\n",
      "Epoch 00016: val_loss did not improve from -0.69547\n",
      "Epoch 17/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: -0.9754 - mean_squared_error: 10.1341 - val_loss: -0.6656 - val_mean_squared_error: 8.6171\n",
      "\n",
      "Epoch 00017: val_loss did not improve from -0.69547\n",
      "Epoch 18/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.9753 - mean_squared_error: 10.4247 - val_loss: -0.6704 - val_mean_squared_error: 23.5880\n",
      "\n",
      "Epoch 00018: val_loss did not improve from -0.69547\n",
      "Epoch 19/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: -0.9767 - mean_squared_error: 10.7087 - val_loss: -0.6729 - val_mean_squared_error: 8.7178\n",
      "\n",
      "Epoch 00019: val_loss did not improve from -0.69547\n",
      "Epoch 20/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 136us/step - loss: -0.9803 - mean_squared_error: 10.9864 - val_loss: -0.6731 - val_mean_squared_error: 8.1173\n",
      "\n",
      "Epoch 00020: val_loss did not improve from -0.69547\n",
      "Epoch 21/100\n",
      "Learning rate:  0.001\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9783 - mean_squared_error: 11.1924 - val_loss: -0.6769 - val_mean_squared_error: 8.9717\n",
      "\n",
      "Epoch 00021: val_loss did not improve from -0.69547\n",
      "Epoch 22/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 115us/step - loss: -0.9844 - mean_squared_error: 11.3241 - val_loss: -0.6852 - val_mean_squared_error: 8.6199\n",
      "\n",
      "Epoch 00022: val_loss did not improve from -0.69547\n",
      "Epoch 23/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 133us/step - loss: -0.9876 - mean_squared_error: 11.3180 - val_loss: -0.6862 - val_mean_squared_error: 8.6074\n",
      "\n",
      "Epoch 00023: val_loss did not improve from -0.69547\n",
      "Epoch 24/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 131us/step - loss: -0.9887 - mean_squared_error: 11.3247 - val_loss: -0.6863 - val_mean_squared_error: 8.5822\n",
      "\n",
      "Epoch 00024: val_loss did not improve from -0.69547\n",
      "Epoch 25/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 4s 112us/step - loss: -0.9892 - mean_squared_error: 11.3107 - val_loss: -0.6864 - val_mean_squared_error: 8.3988\n",
      "\n",
      "Epoch 00025: val_loss did not improve from -0.69547\n",
      "Epoch 26/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: -0.9899 - mean_squared_error: 11.3077 - val_loss: -0.6871 - val_mean_squared_error: 8.4032\n",
      "\n",
      "Epoch 00026: val_loss did not improve from -0.69547\n",
      "Epoch 27/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9901 - mean_squared_error: 11.3095 - val_loss: -0.6865 - val_mean_squared_error: 8.4201\n",
      "\n",
      "Epoch 00027: val_loss did not improve from -0.69547\n",
      "Epoch 28/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 114us/step - loss: -0.9909 - mean_squared_error: 11.2921 - val_loss: -0.6867 - val_mean_squared_error: 8.1559\n",
      "\n",
      "Epoch 00028: val_loss did not improve from -0.69547\n",
      "Epoch 29/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: -0.9911 - mean_squared_error: 11.2819 - val_loss: -0.6870 - val_mean_squared_error: 8.2426\n",
      "\n",
      "Epoch 00029: val_loss did not improve from -0.69547\n",
      "Epoch 30/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9913 - mean_squared_error: 11.2751 - val_loss: -0.6870 - val_mean_squared_error: 8.1820\n",
      "\n",
      "Epoch 00030: val_loss did not improve from -0.69547\n",
      "Epoch 31/100\n",
      "Learning rate:  0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 4s 112us/step - loss: -0.9916 - mean_squared_error: 11.2685 - val_loss: -0.6855 - val_mean_squared_error: 8.3017\n",
      "\n",
      "Epoch 00031: val_loss did not improve from -0.69547\n",
      "Epoch 32/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9918 - mean_squared_error: 11.2724 - val_loss: -0.6862 - val_mean_squared_error: 8.1272\n",
      "\n",
      "Epoch 00032: val_loss did not improve from -0.69547\n",
      "Epoch 33/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 127us/step - loss: -0.9920 - mean_squared_error: 11.2585 - val_loss: -0.6861 - val_mean_squared_error: 8.0043\n",
      "\n",
      "Epoch 00033: val_loss did not improve from -0.69547\n",
      "Epoch 34/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 121us/step - loss: -0.9922 - mean_squared_error: 11.2533 - val_loss: -0.6850 - val_mean_squared_error: 8.2900\n",
      "\n",
      "Epoch 00034: val_loss did not improve from -0.69547\n",
      "Epoch 35/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 133us/step - loss: -0.9922 - mean_squared_error: 11.2474 - val_loss: -0.6840 - val_mean_squared_error: 8.0604\n",
      "\n",
      "Epoch 00035: val_loss did not improve from -0.69547\n",
      "Epoch 36/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: -0.9926 - mean_squared_error: 11.2513 - val_loss: -0.6845 - val_mean_squared_error: 7.8808\n",
      "\n",
      "Epoch 00036: val_loss did not improve from -0.69547\n",
      "Epoch 37/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 122us/step - loss: -0.9928 - mean_squared_error: 11.2516 - val_loss: -0.6840 - val_mean_squared_error: 7.8574\n",
      "\n",
      "Epoch 00037: val_loss did not improve from -0.69547\n",
      "Epoch 38/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: -0.9928 - mean_squared_error: 11.2403 - val_loss: -0.6845 - val_mean_squared_error: 7.9616\n",
      "\n",
      "Epoch 00038: val_loss did not improve from -0.69547\n",
      "Epoch 39/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: -0.9931 - mean_squared_error: 11.2228 - val_loss: -0.6858 - val_mean_squared_error: 8.1149\n",
      "\n",
      "Epoch 00039: val_loss did not improve from -0.69547\n",
      "Epoch 40/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: -0.9930 - mean_squared_error: 11.2259 - val_loss: -0.6855 - val_mean_squared_error: 7.8834\n",
      "\n",
      "Epoch 00040: val_loss did not improve from -0.69547\n",
      "Epoch 41/100\n",
      "Learning rate:  0.0001\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.9932 - mean_squared_error: 11.2209 - val_loss: -0.6853 - val_mean_squared_error: 7.8403\n",
      "\n",
      "Epoch 00041: val_loss did not improve from -0.69547\n",
      "Epoch 42/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 130us/step - loss: -0.9934 - mean_squared_error: 11.2146 - val_loss: -0.6848 - val_mean_squared_error: 7.8400\n",
      "\n",
      "Epoch 00042: val_loss did not improve from -0.69547\n",
      "Epoch 43/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: -0.9935 - mean_squared_error: 11.2094 - val_loss: -0.6852 - val_mean_squared_error: 7.8579\n",
      "\n",
      "Epoch 00043: val_loss did not improve from -0.69547\n",
      "Epoch 44/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.9935 - mean_squared_error: 11.2142 - val_loss: -0.6854 - val_mean_squared_error: 7.9012\n",
      "\n",
      "Epoch 00044: val_loss did not improve from -0.69547\n",
      "Epoch 45/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.9937 - mean_squared_error: 11.2110 - val_loss: -0.6852 - val_mean_squared_error: 7.8301\n",
      "\n",
      "Epoch 00045: val_loss did not improve from -0.69547\n",
      "Epoch 46/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 4s 111us/step - loss: -0.9936 - mean_squared_error: 11.2156 - val_loss: -0.6851 - val_mean_squared_error: 7.8341\n",
      "\n",
      "Epoch 00046: val_loss did not improve from -0.69547\n",
      "Epoch 47/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 129us/step - loss: -0.9936 - mean_squared_error: 11.2078 - val_loss: -0.6851 - val_mean_squared_error: 7.8605\n",
      "\n",
      "Epoch 00047: val_loss did not improve from -0.69547\n",
      "Epoch 48/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: -0.9937 - mean_squared_error: 11.2108 - val_loss: -0.6848 - val_mean_squared_error: 7.8300\n",
      "\n",
      "Epoch 00048: val_loss did not improve from -0.69547\n",
      "Epoch 49/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 118us/step - loss: -0.9937 - mean_squared_error: 11.2065 - val_loss: -0.6851 - val_mean_squared_error: 7.8504\n",
      "\n",
      "Epoch 00049: val_loss did not improve from -0.69547\n",
      "Epoch 50/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 131us/step - loss: -0.9937 - mean_squared_error: 11.2111 - val_loss: -0.6850 - val_mean_squared_error: 7.8604\n",
      "\n",
      "Epoch 00050: val_loss did not improve from -0.69547\n",
      "Epoch 51/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 130us/step - loss: -0.9936 - mean_squared_error: 11.2095 - val_loss: -0.6850 - val_mean_squared_error: 7.9571\n",
      "\n",
      "Epoch 00051: val_loss did not improve from -0.69547\n",
      "Epoch 52/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 117us/step - loss: -0.9938 - mean_squared_error: 11.2002 - val_loss: -0.6854 - val_mean_squared_error: 7.8429\n",
      "\n",
      "Epoch 00052: val_loss did not improve from -0.69547\n",
      "Epoch 53/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 126us/step - loss: -0.9939 - mean_squared_error: 11.2027 - val_loss: -0.6852 - val_mean_squared_error: 7.7839\n",
      "\n",
      "Epoch 00053: val_loss did not improve from -0.69547\n",
      "Epoch 54/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 128us/step - loss: -0.9938 - mean_squared_error: 11.2080 - val_loss: -0.6853 - val_mean_squared_error: 7.7122\n",
      "\n",
      "Epoch 00054: val_loss did not improve from -0.69547\n",
      "Epoch 55/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 116us/step - loss: -0.9938 - mean_squared_error: 11.2000 - val_loss: -0.6850 - val_mean_squared_error: 7.8545\n",
      "\n",
      "Epoch 00055: val_loss did not improve from -0.69547\n",
      "Epoch 56/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 132us/step - loss: -0.9939 - mean_squared_error: 11.2075 - val_loss: -0.6851 - val_mean_squared_error: 7.7762\n",
      "\n",
      "Epoch 00056: val_loss did not improve from -0.69547\n",
      "Epoch 57/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 133us/step - loss: -0.9939 - mean_squared_error: 11.2085 - val_loss: -0.6853 - val_mean_squared_error: 7.8546\n",
      "\n",
      "Epoch 00057: val_loss did not improve from -0.69547\n",
      "Epoch 58/100\n",
      "Learning rate:  1e-05\n",
      "40000/40000 [==============================] - 5s 120us/step - loss: -0.9939 - mean_squared_error: 11.2048 - val_loss: -0.6855 - val_mean_squared_error: 7.9165\n",
      "\n",
      "Epoch 00058: val_loss did not improve from -0.69547\n",
      "Epoch 59/100\n",
      "Learning rate:  1e-05\n",
      "23040/40000 [================>.............] - ETA: 2s - loss: -0.9938 - mean_squared_error: 11.2018"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-977d62f2d6c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m          )\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train_feat, y_train_embeddings,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test_seen_feat, y_test_seen_embeddings),\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks,\n",
    "          verbose=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top K Predictions\n",
    "def pred_top_k(y_test_pred, label_embeddings_arr=label_embeddings_arr, k=5):\n",
    "    sim_table = cosine_similarity(y_test_pred, label_embeddings_arr)\n",
    "    top_k_guesses = np.argpartition(sim_table,range(99-k+1,100),axis=1)[:,99-k+1:]\n",
    "    return sim_table, top_k_guesses\n",
    "\n",
    "#Top k Accuracy\n",
    "def calc_top_k_acc(top_k, y_test):\n",
    "    correct = 0\n",
    "    for i in range(y_test.shape[0]):\n",
    "        if np.squeeze(y_test)[i] in top_k[i]:\n",
    "            correct += 1\n",
    "    return correct/float(y_test.shape[0])\n",
    "\n",
    "def evaluate(model, x_test, y_test, label_embeddings_arr, k=5):\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    sim_table, top_k_guesses = pred_top_k(y_test_pred, label_embeddings_arr, k=k)\n",
    "\n",
    "    #Top prediction\n",
    "    label_predictions = sim_table.argmax(axis=1)\n",
    "    \n",
    "    #Accuracy\n",
    "    acc = np.sum((np.squeeze(y_test) == label_predictions)) / float(y_test.shape[0])\n",
    "    top_k_acc = calc_top_k_acc(top_k_guesses, y_test)\n",
    "    print(\"Accuracy: \" + str(acc))\n",
    "    print(\"Top \" + str(k) + \" Accuracy: \" + str(top_k_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_filepath = os.path.join(save_dir, 'cifar100_%s_w2v_transfer_model.%03d.h5' % (model_type, 4))\n",
    "best_model = load_model(best_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.505125\n",
      "Top 5 Accuracy: 0.682375\n"
     ]
    }
   ],
   "source": [
    "evaluate(best_model, X_test_seen_feat, y_test_seen, label_embeddings_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.333333333333333e-05\n",
      "Top 5 Accuracy: 0.16975\n"
     ]
    }
   ],
   "source": [
    "# Although accuracy is still 0%, we seen an improvement in top 5 % accuracy (0% -> 15%). Sign that w2v is useful for ZSL\n",
    "\n",
    "evaluate(best_model, X_test_unseen_feat, y_test_unseen, label_embeddings_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.23166666666666666\n",
      "Top 5 Accuracy: 0.4994166666666667\n"
     ]
    }
   ],
   "source": [
    "# Regular ZSL setting where we only need to consider choose between the 20 unseen classes, rather than all 100\n",
    "\n",
    "# Just replace the word vectors for \"seen\" labels with something really far so it won't be close to any predicted vector\n",
    "unseen_labels = np.unique(y_test_unseen)\n",
    "label_embeddings_arr_unseen = np.copy(label_embeddings_arr)\n",
    "for i in range(100):\n",
    "    if i in unseen_labels:\n",
    "        continue\n",
    "    label_embeddings_arr_unseen[i] = np.ones(label_embeddings_arr[0].shape) * 1000\n",
    "\n",
    "evaluate(best_model, X_test_unseen_feat, y_test_unseen, label_embeddings_arr_unseen) # Significant improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x = np.linspace(0,99,100) \n",
    "y = np.zeros(100)\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "for i in range(100):\n",
    "    k = i+1\n",
    "    sim_table, top_k_guesses = pred_top_k(y_test_pred, k=k)\n",
    "    y[i] = calc_top_k_acc(top_k_guesses, y_test)\n",
    "    \n",
    "plt.plot(x,y)\n",
    "plt.title(\"Top K Accuracy\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
